{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.offline import iplot"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unix_to_datetime_local(df):\n",
    "    time_sampled = list(df['time_sampled'])\n",
    "    datetime_obj = []\n",
    "    for i in time_sampled:\n",
    "        datetime_obj.append(datetime.fromtimestamp(int(i))) #converts each timestamp to datetime format using computer's local timezone ie KHI\n",
    "    df['datetime'] = datetime_obj\n",
    "    return df\n",
    "\n",
    "def vol_in_interval(df):\n",
    "    total_flow = list(df['total_flow'])\n",
    "    flow_delta = []\n",
    "\n",
    "    for i in range(len(total_flow)):\n",
    "        if i != len(total_flow)-1:\n",
    "            flow_delta.append(float(total_flow[i]) - float(total_flow[i+1]))\n",
    "        else:\n",
    "            flow_delta.append(0)\n",
    "\n",
    "    df['volume'] = flow_delta\n",
    "    return df\n",
    "\n",
    "def rem_nonzero_start(df):\n",
    "    totalflow = list(df['total_flow'])\n",
    "    totalflow.reverse()\n",
    "    for i in range(len(totalflow)):\n",
    "        if totalflow[i] == 0:\n",
    "            break\n",
    "    df = df.iloc[:len(totalflow)-i]\n",
    "    return df\n",
    "\n",
    "def positive_vol(df):\n",
    "    df = df[df['volume']>=0]\n",
    "    return df \n",
    "\n",
    "def abs_vol(df):\n",
    "    abs_vol = []\n",
    "    for v in list(df['volume']):\n",
    "        if v < 0:\n",
    "            abs_vol.append(abs(v))\n",
    "        else:\n",
    "            abs_vol.append(v)\n",
    "    df['volume'] = abs_vol\n",
    "    return df\n",
    "\n",
    "def clean_flowrate(df, maxflow):\n",
    "    '''Removing datapoints where flowrate exceeds sensor capacity.'''\n",
    "    df = df[df['flow_rate'] <= maxflow]\n",
    "    # df = df[df['flow_rate'] > 0] #remove 0's\n",
    "    return df  \n",
    "\n",
    "def add_days(df):\n",
    "    '''Storing day names for processing each week'''\n",
    "    time_sampled = list(df['time_sampled'])\n",
    "    days = []\n",
    "    for i in time_sampled:\n",
    "        days.append(datetime.fromtimestamp(int(i)).strftime('%A')[:2])\n",
    "    df['day'] = days\n",
    "    return df\n",
    "\n",
    "def get_cum_vol(df):\n",
    "    vol =df['volume']\n",
    "    vol = vol.iloc[::-1]\n",
    "    cumvol = vol.cumsum()\n",
    "    cumvol = cumvol.iloc[::-1]\n",
    "    df['cumvol'] = cumvol\n",
    "    return df\n",
    "\n",
    "def add_date_time(df):\n",
    "    list_time = [i.time() for i in  list(df['datetime'])]\n",
    "    list_date = [i.date() for i in  list(df['datetime'])]\n",
    "    df['date'] = list_date\n",
    "    df['time'] = list_time\n",
    "    return df\n",
    "\n",
    "def get_daily_cum_vol(df):\n",
    "    daily_vol = []\n",
    "    dates = df['date'].unique()\n",
    "    for d in dates:\n",
    "        df_date = df[df[\"date\"] == d]\n",
    "        vol =df_date['volume']\n",
    "        # vol = vol.iloc[::-1]\n",
    "        cumvol = vol.cumsum()\n",
    "        # cumvol = cumvol.iloc[::-1]\n",
    "        daily_vol.extend(list(cumvol))\n",
    "    df['DVol'] = daily_vol\n",
    "    return df\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_max = pd.read_csv('D:\\OneDrive - Habib University\\HU\\KWP\\Data\\InstalledNodes.csv')\n",
    "max_node_capacity = list(df_max['MaxFlow'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,10):\n",
    "    df = pd.read_csv('D:\\OneDrive - Habib University\\HU\\KWP\\Data\\ALLSENSORDATA\\\\new\\\\N'+str(i)+'.csv')\n",
    "    df = df[['time_sampled','flow_rate']]\n",
    "    print('Node:',i)\n",
    "    og_len = len(df)\n",
    "    print('og:',og_len)\n",
    "    df = unix_to_datetime_local(df)\n",
    "    df = rem_nonzero_start(df)\n",
    "    print('removed non-0 start:',len(df))\n",
    "    df = vol_in_interval(df)\n",
    "    df = abs_vol(df)\n",
    "    df = clean_flowrate(df,max_node_capacity[i-1])\n",
    "    print('removed flow values:',len(df))\n",
    "    df = add_days(df)\n",
    "    # df = get_cum_vol(df)\n",
    "    print(len(df))\n",
    "    print('datapoints removed:',(og_len - len(df)))\n",
    "    # df = add_date_time(df)\n",
    "    # df = get_daily_cum_vol(df)\n",
    "    df = df.drop(columns=['total_flow'])\n",
    "    df = df.iloc[::-1]\n",
    "    df.insert(0,'NodeID','N'+str(i))\n",
    "    df.to_csv('Cleaned/N'+str(i)+'_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data[0]\n",
    "df = df.set_index('datetime')\n",
    "grouped_data = df.groupby(df.index.date).max()\n",
    "fig = px.line(grouped_data, x=grouped_data.index, y=['flow_rate','volume'],title=\"N02 - Processed\",category_orders={'day':['Mo','Tu','We','Th','Fr','Sa','Su']})\n",
    "fig.for_each_annotation(lambda a: a.update(text=a.text.split(\"=\")[-1]))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Adding DVol to preprocessed data'''\n",
    "for i in range(10,27):\n",
    "    df = pd.read_csv('D:\\OneDrive - Habib University\\HU\\KWP\\\\Data\\Preprocessed\\\\N'+str(i)+'_cleaned.csv')\n",
    "    df['date'] = df.datetime.str[:10]\n",
    "    df = get_daily_cum_vol(df)\n",
    "    df = df.drop(columns=['Unnamed: 0','date'])\n",
    "    df.to_csv('D:\\OneDrive - Habib University\\HU\\KWP\\\\Data\\Preprocessed\\with dvol\\\\N'+str(i)+'_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Generating TF Files'''\n",
    "for i in range(10,27):\n",
    "    df_tf = pd.DataFrame(columns=['NodeID','Date','TotalVol','No_dp'])\n",
    "    df = pd.read_csv('D:\\OneDrive - Habib University\\HU\\KWP\\\\Data\\Preprocessed\\with dvol\\\\N'+str(i)+'_cleaned.csv')\n",
    "    df['Date'] = df.datetime.str[:10]\n",
    "    temp = df.groupby('Date').max()\n",
    "    df_tf['NodeID'] = temp['NodeID']\n",
    "    df_tf['TotalVol'] = temp['DVol']\n",
    "    df_tf['Date'] = temp.index\n",
    "    df_tf['No_dp'] = df.groupby('Date').size()\n",
    "    df_tf.to_csv('D:\\OneDrive - Habib University\\HU\\KWP\\\\Data\\Preprocessed\\\\TF_N'+str(i)+'.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d17d789d09319aec541bff4a4c00f3ea3849f8cb7bec3b7b901fb7a4cea7bb15"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
